# -*- coding: utf-8 -*-
"""160524_H_and_S_project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zSGf4PGQJ4uXKuabcr2vMUbsJI2GrHRP
"""

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

!mkdir ~/.kaggle

!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json

! kaggle datasets download junyiacademy/learning-activity-public-dataset-by-junyi-academy

! unzip learning-activity-public-dataset-by-junyi-academy.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_InfoUser = pd.read_csv('Info_UserData.csv')
df_LogProblem = pd.read_csv('Log_Problem.csv')
df_InfoContent = pd.read_csv('Info_Content.csv')

# Join tables based on uuid and ucid
#df_one = pd.merge(df_LogProblem, df_InfoUser, on='uuid')
#df_merged = pd.merge(df_one, df_InfoContent, on='ucid')

# I tried to merge the datasets but the session crashed repeatedly, therefore I performed an EDA separately and only
# joined to df_LogProblem the needed variables from the other datasets through uuid and ucid

"""**Let's analyse InfoUser dataset**"""

df_InfoUser.head(10)

df_InfoUser.info()

# Check for missing values
missing_values = df_InfoUser.isnull().sum()

print("Missing values in each column:")
print(missing_values)

"""**We only have null values for gender variable, let's substitute them with "unspecified"**"""

df_InfoUser = df_InfoUser.fillna("unspecified")
counting_each_gender = df_InfoUser['gender'].value_counts()
counting_each_gender

colors = plt.cm.viridis(np.linspace(0, 1, len(counting_each_gender)))
plt.title("Number of students per gender")
plt.bar(counting_each_gender.index, counting_each_gender.values, color=colors)
plt.show()

df_InfoUser['first_login_date_TW'] = pd.to_datetime(df_InfoUser['first_login_date_TW'])

df_InfoUser.dtypes

df_InfoUser.describe()

"""**Observations:**
1. We can see from the above data, that on average the user grade is 5.6 and the badges and points earned are respectively 9.5 and 63.047;
2. 75% of the students have a grade below 7;
3. The mean could be influenced by extreme values therefore it's better to consider the median which is 6.0 for the grade (50% percentile)

**Let's study correlations among the variables in InfoUser**
"""

df_InfoUser_copy = df_InfoUser.copy()
columns = ['points','badges_cnt', 'user_grade', 'has_teacher_cnt',
           'is_self_coach', 'has_student_cnt', 'belongs_to_class_cnt', 'has_class_cnt']

correlation = df_InfoUser_copy[columns].corr()
correlation

plt.figure(figsize = (6, 6))
sns.heatmap(correlation,
            xticklabels=correlation.columns.values,
            yticklabels=correlation.columns.values, annot=True)

"""**We only have an high correlation between points and badges**

**Let's detect outliers for the grade variable**
"""

# detect outliers
#plt.figure(figsize=(10, 6))
#plt.boxplot(df_InfoUser['user_grade'], vert=False)
#plt.xlabel('Grade')
#plt.title('Box Plot of Student Grades')
#plt.show()

"""**12 is an outlier for the grade**"""

import seaborn as sns
from matplotlib import pyplot as plt

palette = sns.color_palette("husl", 12)

plt.figure(figsize=(20,5))
g = sns.countplot(x='user_grade', data = df_InfoUser, palette=palette)
g.set(xlabel='Grade', ylabel='Frequency',title = 'The distribution of the number of unique students by grade')
plt.show()

# the code below creates the same plot with mathplot.pyplot:

# Calculate the number of users per grade
#grade_counts = df_InfoUser['user_grade'].value_counts().sort_index()

# Plot a bar plot
#plt.figure(figsize=(10, 6))
#grade_counts.plot(kind='bar')
#plt.xlabel('Grade')
#plt.ylabel('Number of Users')
#plt.title('Number of Users per Grade')
#plt.show()

# grade by gender
average_grade_per_gender = df_InfoUser.groupby('gender')['user_grade'].mean().reset_index()

# Rename the columns for clarity
average_grade_per_gender.columns = ['Gender', 'Average Grade']

# Display the result
print(average_grade_per_gender)

"""**There is not much difference with respect to gender**

**Let's remove all users with 12 as grade because this value is an outlier**
"""

new_df_InfoUser = df_InfoUser[df_InfoUser['user_grade']<12]
new_df_InfoUser.shape

# Calculate the distribution of badge counts
badge_count_distribution = df_InfoUser['badges_cnt'].value_counts().sort_index()

# Plot a histogram for badge count distribution using Seaborn
plt.figure(figsize=(14, 14))
sns.histplot(df_InfoUser['badges_cnt'], bins=100)
plt.xlabel('Number of Badges')
plt.ylabel('Number of Students')
plt.title('Distribution of Number of Badges per Student')
plt.show()

# most frequent numbers of badges
badge_freq_table = df_InfoUser['badges_cnt'].value_counts().sort_index().reset_index()
badge_freq_table.columns = ['Number of Badges', 'Frequency']
badge_freq_table['Percentage'] = round((badge_freq_table['Frequency'] / badge_freq_table['Frequency'].sum()) * 100,2)

print("Frequency Table for Number of Badges:")
print(badge_freq_table.head(20))

#lineplot versione of the graph
plt.figure(figsize=(20, 5))
g = sns.lineplot(data=badge_freq_table, x='Number of Badges', y='Frequency')
g.set(xlabel='Number of Badges', ylabel='Frequency', title='Distribution of Number of Badges per Student')
plt.show()

"""**We can see that around 48% of students have between 0 and 2 badges, therefore instead of the value 9.5 as average number of badges that we Considered before (mean calculated through .describe()) it's better to consider the median of 3.**

**Let's remove all users with a number of badges lower 10, to reduce the size of the dataset**
"""

new_df_InfoUser = new_df_InfoUser[new_df_InfoUser['badges_cnt']<11]
new_df_InfoUser.shape

new_df_InfoUser.isnull().sum()

"""Let's see the distribution of all numarical variable in our user Info dataset for outliers.

If we standardize data we'll have problem after we merge the 3 datasets and try to analyze final_merged_df
"""

#df = df_InfoUser.copy()
### only taking numerical features into account
#columns = ['points','badges_cnt', 'user_grade', 'has_teacher_cnt',
#             'has_student_cnt', 'belongs_to_class_cnt', 'has_class_cnt']
#
#for column in columns:                                                                                             #### column standardising
#    df[column] = df[column].apply(lambda x :( (x-df[column].mean())/(df[column].max()-df[column].min())))

# detect outliers
#columns.remove('user_grade')
#plt.figure(figsize=(8,8))
#sns.boxplot(df[columns])
#plt.xticks(rotation  = 90)
#plt.show()

"""Observations:

All the variables have extreme points which could be outliers

let's see the violin plot
"""

#plt.figure(figsize = (8,8))
#sns.violinplot(data = df[columns])
#
#plt.xticks(rotation  = 90)
#plt.show()

#df[columns].describe()

"""Observation:

There is a huge difference between 75 percentile and max values of the features.
We need to remove the outliers using IQR method
"""

#for colmn in columns:                                                           ### using IQR to remove outliers
#  Q3 = df[colmn].quantile(0.75)                                                  ## quantile 3
#  Q1 = df[colmn].quantile(0.25)                                                 ## quantile 1
#  IQR = Q3-Q1                                                                     ## inter quantile range
#  df = df[(df[colmn]>= Q1-IQR) & (df[colmn]<= Q3+IQR)]

#df.shape

"""**InfoContent table analysis**"""

df_InfoContent.head()

df_InfoContent.dtypes

df_InfoContent.describe()

df_InfoContent.isnull().sum()

"""**Contains no null values.Lets seee different diffulty levels and learning stages**"""

df_1 = pd.DataFrame(df_InfoContent['learning_stage'].value_counts())
plt.figure(figsize=(6, 4))
sns.barplot(data = df_1, x='learning_stage' , y = 'count' ,palette=sns.color_palette("hsv", len(df_1.index)))
plt.title('Count of Categories')
plt.xlabel('Learning_Stages')
plt.ylabel('Count')
plt.show()

df_InfoContent.groupby(by=["difficulty", "learning_stage"], as_index=False).size()

# graph rapresentation of frequency table
import plotly.offline as pyo
import plotly.graph_objs as go
lowerdf = df_InfoContent.groupby('difficulty').size()/df_InfoContent['learning_stage'].count()*100
labels = lowerdf.index
values = lowerdf.values

# Use `hole` to create a donut-like pie chart
fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.6)])
fig.show()

"""1. Most of the content belongs to elementary and least belongs to senior.

2. 62.8% of the content are easy and 3% are unset

**Analysis of LogProblem dataset**
"""

df_LogProblem.head(10)

df_LogProblem.shape

df_LogProblem.describe()

"""the users solved 10 problems on average (median value is 6 instead)"""

# to also describe categorical variables
df_LogProblem.describe(include = 'object')

df_LogProblem.info()

# Check for missing values
missing_values = df_LogProblem.isnull().sum()

print("Missing values in each column:")
print(missing_values)

df_LogProblem[['is_downgrade', 'is_upgrade']].isnull().mean() * 100

"""Arrounf 89 percentage of is_downgrade and is_upgrade values are null.

**Let's reduce LogProblem size to merge the 3 datasets:**

1. We remove all of the rows with a problem_number value lower than 10 because the majority of the users have solved max 10 problems

2. We remove all of the rows where we have a null value therefore the rows of is_upgrade, is_downgrade where we have a null value
"""

new_df_LogProblem = df_LogProblem[df_LogProblem['problem_number']<11]
new_df_LogProblem.shape

new_df_LogProblem = df_LogProblem.dropna()
new_df_LogProblem.shape

merged_df = pd.merge(new_df_LogProblem, new_df_InfoUser, on='uuid', how='left')
merged_df.shape

final_merged_df = pd.merge(merged_df, df_InfoContent, on='ucid', how='left')
final_merged_df.shape

final_merged_df.head()

final_merged_df.dtypes

cols = ['points','badges_cnt','user_grade','has_teacher_cnt','belongs_to_class_cnt',
                                   'has_student_cnt','has_class_cnt']
final_merged_df[cols] = final_merged_df[cols].fillna(0).astype(int)

final_merged_df.dtypes

final_merged_df['timestamp_TW'] = pd.to_datetime(final_merged_df['timestamp_TW'])

final_merged_df['timestamp_TW'] = pd.to_datetime(final_merged_df['timestamp_TW'],format= '%d/%m/%y').dt.date

final_merged_df['timestamp_TW'] = pd.to_datetime(final_merged_df['timestamp_TW'])

final_merged_df.dtypes

final_merged_df.head()

final_merged_df.isnull().sum()

final_merged_df['gender'] = final_merged_df['gender'].fillna("unspecified")

final_merged_df.isnull().sum()

final_merged_df = final_merged_df.drop(columns = ['content_pretty_name'])

final_merged_df.isnull().sum()

"""Now, Let's get some insights from merged dataset"""

df_elementary = final_merged_df[final_merged_df['learning_stage'] == "elementary"]
df_junior = final_merged_df[final_merged_df['learning_stage'] == "junior"]
df_senior = final_merged_df[final_merged_df['learning_stage'] == "senior"]

plt.figure(figsize=(10, 6))

plt.plot(df_elementary['points'].sort_values().reset_index(drop=True) , label = 'elementary')
plt.plot(df_junior['points'].sort_values().reset_index(drop=True) , label = 'junior')
plt.plot(df_senior['points'].sort_values().reset_index(drop=True) , label = 'senior')

# adding titles and labels
plt.title('Distribution of Various Metrics for Students at different learning stages', fontsize=16)
plt.xlabel('Student sorted by energy point count', fontsize=10)
plt.ylabel('Energy points', fontsize=10)

# Adding grid, legend, and show plot
plt.grid()
plt.legend()
plt.show()

plt.plot(df_senior['points'].sort_values().reset_index(drop=True))

plt.title('Distribution of energy points for students at senior stages', fontsize=16)
plt.xlabel('Student sorted by energy point count', fontsize=10)
plt.ylabel('Energy points', fontsize=10)
plt.grid()

plt.show()

"""Both above plot shows that the there is a significant difference between users frequency. Some are very frequent users whereas others are not

Let's see the number of problems done by users
"""

# Calculate number of problems done by each user
df_pcnt = final_merged_df.groupby('uuid').size().reset_index(name='problem_cnt')
df_pcnt = df_pcnt.sort_values(by=['problem_cnt'])
df_pcnt = df_pcnt.reset_index()

# Sort and plot
pcnt_distribution = df_pcnt['problem_cnt'].value_counts()
pcnt_distribution = pcnt_distribution.sort_index()

plt.bar(pcnt_distribution.index, pcnt_distribution.values)

plt.title('Distribution of problem attempts for students in this exercise', fontsize=16)
plt.xlabel("Number of problems done in this exercise", fontsize=10)
plt.ylabel("User count", fontsize=10)
plt.xlim((0, 30))

plt.show()

"""We can see that the number of users versus number of problems done in this exercise is a reducing plot. Means Most of the users are at level_1, then level_2 then level_3 then level_4

Let's try to analyse the user learning path
"""

# Lets randomly pick a user and an exercise and observe the learning process!
learning_path = final_merged_df[(final_merged_df['uuid'] == "prmwmW2vzHAjfG8oG3eNSuZBSgBlQ1Uv2FKAdjEA8J0=") &
                              (final_merged_df['ucid'] == "tOR47i7wGbuHkGxkhX9tHItMHHuJtAttsvHDOjDRVOw=")]

#sort by problem_number
learning_path = learning_path.sort_values(by=['problem_number']).reset_index()
learning_path = learning_path[['timestamp_TW', 'upid', 'problem_number', 'exercise_problem_repeat_session', 'is_correct']]
learning_path

"""One exercise contatains many problems and the exercise_problem_repeat_session is the number of times a user is encounteres this problem.

Lets see if the correct rate is related to difficulty level.
"""

df_Problem_Content = final_merged_df[final_merged_df['difficulty'] != 'unset']
df_Problem_Content = df_Problem_Content.groupby(['difficulty', 'is_correct']).size().unstack(level=-1)
df_Problem_Content['correct_rate'] = df_Problem_Content[True] / (df_Problem_Content[True] + df_Problem_Content[False])
df_Problem_Content = df_Problem_Content.sort_values(by=['correct_rate'], ascending=False)

# Reset the index for plotting
df_Problem_Content = df_Problem_Content.reset_index()

# Plot the data using seaborn
plt.figure(figsize=(8, 4))
sns.barplot(x='difficulty', y='correct_rate', data=df_Problem_Content, palette='viridis')

# Add titles and labels
plt.title('Correct Rate by Difficulty Level', fontsize=16)
plt.xlabel('Difficulty Level', fontsize=12)
plt.ylabel('Correct Rate', fontsize=12)

# Show plot
plt.show()

"""There is not much difference in Correct rates for different Difficulty levels

Let's see what are the top 10 user cities
"""

top_10_cities = final_merged_df['user_city'].value_counts().head(10)

# Create a bar plot
plt.figure(figsize=(8, 4))
ax = sns.barplot(x=top_10_cities.values, y=top_10_cities.index, palette='viridis')

# Add titles and labels
plt.title('Top 10 Cities by Number of Users', fontsize=16)
plt.xlabel('Number of Users', fontsize=12)
plt.ylabel('City', fontsize=12)

"""Let,s do feature engineering, notmalisation, outlier removal and important feature selection"""

final_merged_df.info()

# Select only numeric columns
numeric_columns = final_merged_df.select_dtypes(include=['number']).columns
df_numeric = final_merged_df[numeric_columns]

# Calculate correlation matrix
corr = df_numeric.corr()

# Display the correlation matrix
print(corr)

plt.figure(figsize = (7, 7))
sns.heatmap(corr,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

"""1. As we can see from the correlation matrix, there is an high level of correlation between points, badges and user grade, meaning that when the user's points and/or badges increase the user's grade increases too.

2. Moreover, the the number of points, badges and the user grade are also positively correlated to if the student has a teacher or not: When the student has a teacher, there is an higher chance they have an higher number of points, badges and grade.
"""

final_merged_df['gender'].replace({'unspecified': 0, 'male': 1, 'female': 2},inplace=True)
final_merged_df['difficulty'].replace({'unset': 0, 'easy': 1, 'normal': 2, 'hard': 3}, inplace=True)
final_merged_df['learning_stage'].replace({'elementary': 0, 'junior': 1, 'senior': 2}, inplace=True)
final_merged_df['is_correct'].replace({'True': 1, 'False': 0,}, inplace=True)

# Create X and y objects
X = final_merged_df[['points', 'exercise_problem_repeat_session','badges_cnt','has_teacher_cnt','total_sec_taken','total_attempt_cnt',
                     'used_hint_cnt','is_hint_used','problem_number','has_class_cnt','level','gender','difficulty','learning_stage','is_correct']]
y = final_merged_df['user_grade']

# Create Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345,shuffle=True)

# Fit model
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

my_lm = LinearRegression()
my_lm.fit(X = X_train, y = y_train)

train_fcst = my_lm.predict(X_train)
test_fcst = my_lm.predict(X_test)

train_r2 = r2_score(y_train, train_fcst)
test_r2 = r2_score(y_test, test_fcst)

print(f'Train R-squared: {train_r2}')
print(f'Test R-squared: {test_r2}')

# Plot result
plt.figure(figsize=(10,5))
plt.plot(list(test_fcst), label='Predicted')
plt.plot(list(y_test), label='Actual')
plt.xlabel('Steps into the test set')
plt.ylabel('User grade levels')
plt.title('Comparison of Predicted and Actual values for user_grade')
plt.legend()
plt.show()

"""To do:
1. drop na to improve model, (tried but not working, the predictive power of the model drops at 1,8%)

2. modify difficulty, gender and some other variable to make them numeric in order to improve model

3. consider a bigger merged dataset (we only have 1.7 M values out of 16 M)

We only consider 1000 samples to improve the visualization of the results
"""

subset_size = 1000  # Adjust this value as needed
plt.figure(figsize=(12,6))
plt.plot(list(test_fcst[:subset_size]), label='Predicted')
plt.plot(list(y_test[:subset_size]), label='Actual')
plt.xlabel('Steps into the test set')
plt.ylabel('Correct user grade levels')
plt.legend()
plt.title('Comparison of Predicted and Actual User Grades (Subset)')
plt.show()

plt.figure(figsize=(8,8))
plt.scatter(test_fcst, y_test, alpha=0.5)
plt.xlabel('Predicted User Grade')
plt.ylabel('Actual User Grade')
plt.title('Predicted vs Actual User Grades')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')  # Line of perfect prediction
plt.show()

# Create X and y objects
X = final_merged_df[['points', 'exercise_problem_repeat_session','badges_cnt','has_teacher_cnt','total_sec_taken','total_attempt_cnt',
                     'used_hint_cnt','is_hint_used','problem_number','has_class_cnt','level','gender','difficulty','learning_stage','user_grade']]
y = final_merged_df['is_correct']

# One-hot encoding for categorical variables
X = pd.get_dummies(X, columns=['gender', 'difficulty', 'learning_stage'], drop_first=True)

# Create Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345, shuffle=True)

# Fit model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score

# Logistic Regression is more appropriate for binary classification
my_lm = LogisticRegression(max_iter=1000)
my_lm.fit(X_train, y_train)

# Predict on training and test set
train_preds = my_lm.predict(X_train)
test_preds = my_lm.predict(X_test)

# Evaluate the model
train_accuracy = accuracy_score(y_train, train_preds)
test_accuracy = accuracy_score(y_test, test_preds)
train_precision = precision_score(y_train, train_preds)
test_precision = precision_score(y_test, test_preds)
train_recall = recall_score(y_train, train_preds)
test_recall = recall_score(y_test, test_preds)
train_f1 = f1_score(y_train, train_preds)
test_f1 = f1_score(y_test, test_preds)
test_roc_auc = roc_auc_score(y_test, test_preds)

print(f'Train Accuracy: {train_accuracy}')
print(f'Test Accuracy: {test_accuracy}')
print(f'Train Precision: {train_precision}')
print(f'Test Precision: {test_precision}')
print(f'Train Recall: {train_recall}')
print(f'Test Recall: {test_recall}')
print(f'Train F1 Score: {train_f1}')
print(f'Test F1 Score: {test_f1}')
print(f'Test ROC AUC: {test_roc_auc}')

# Confusion matrix
conf_matrix = confusion_matrix(y_test, test_preds)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Plot ROC curve
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, my_lm.predict_proba(X_test)[:,1])
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % test_roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""Logistic Regression model is a very good one for prediction, let's try with decision trees."""

import numpy as np
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Define X and y
X = final_merged_df[['points', 'problem_number', 'level', 'difficulty', 'learning_stage', 'user_grade']]
y = final_merged_df['is_correct']

# Create Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345, shuffle=False)

# Initialize the Decision Tree Classifier with specific parameters to regularize the tree
dtree = DecisionTreeClassifier(
    max_depth=10,                # Limits depth of the tree
    min_samples_split=15,       # Requires at least 15 samples to consider a split
    min_samples_leaf=5,         # Requires at least 5 samples per leaf
    max_leaf_nodes=200,         # Maximum number of leaf nodes
    random_state=12345)        # Ensures reproducibility
dtree.fit(X_train, y_train)

# Make predictions
y_pred_train = dtree.predict(X_train)
y_pred_test = dtree.predict(X_test)

# Evaluate the model
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
train_precision = precision_score(y_train, y_pred_train)
test_precision = precision_score(y_test, y_pred_test)
train_recall = recall_score(y_train, y_pred_train)
test_recall = recall_score(y_test, y_pred_test)
train_f1 = f1_score(y_train, y_pred_train)
test_f1 = f1_score(y_test, y_pred_test)
test_roc_auc = roc_auc_score(y_test, y_pred_test)

print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")
print(f"Train Precision: {train_precision}")
print(f"Test Precision: {test_precision}")
print(f"Train Recall: {train_recall}")
print(f"Test Recall: {test_recall}")
print(f"Train F1 Score: {train_f1}")
print(f"Test F1 Score: {test_f1}")
print(f"Test ROC AUC: {test_roc_auc}")

# Classification report
print("Classification Report:")
print(classification_report(y_test, y_pred_test))

# Plot confusion matrix
ConfusionMatrixDisplay.from_estimator(dtree, X_test, y_test)
plt.title("Confusion Matrix")
plt.show()

# Visualize the Decision Tree
plt.figure(figsize=(20, 10))
tree.plot_tree(dtree, feature_names=X.columns, class_names=['Incorrect', 'Correct'], filled=True)
plt.show()

# Get the cost complexity pruning path
path = dtree.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# Plot the ccp_alpha vs impurities
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
plt.xlabel("effective alpha")
plt.ylabel("total impurity of leaves")
plt.title("Total Impurity vs effective alpha for training set")
plt.show()

#from sklearn.model_selection import cross_val_score
#
#alphas = path.ccp_alphas[:-1]  # exclude the maximum alpha value
#
#dt_scores = []
#
#for alpha in alphas:
#    dt = DecisionTreeClassifier(random_state=12345, ccp_alpha=alpha)
#    scores = cross_val_score(dt, X_train, y_train, cv=5)
#    dt_scores.append(scores.mean())
#
#optimal_alpha = alphas[np.argmax(dt_scores)]
#
#

"""Cross validation to find best alpha is extremely slow therefore we are considering the tenth alpha form the graph."""

# You might choose an alpha value based on this plot or further analysis
# Here we select an alpha somewhere from the plot - this would be updated based on analysis
alpha_selected = ccp_alphas[10]  # Example: select the 10th alpha

# Re-train the tree with the selected alpha
my_dt_pruned = DecisionTreeClassifier(
    random_state=44,
    max_depth=10,
    min_samples_split=15,
    min_samples_leaf=5,
    max_leaf_nodes=20,
    ccp_alpha=alpha_selected
)
my_dt_pruned.fit(X_train, y_train)

# Make predictions
y_pred_train = dtree.predict(X_train)
y_pred_test = dtree.predict(X_test)

# Evaluate the model
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
train_precision = precision_score(y_train, y_pred_train)
test_precision = precision_score(y_test, y_pred_test)
train_recall = recall_score(y_train, y_pred_train)
test_recall = recall_score(y_test, y_pred_test)
train_f1 = f1_score(y_train, y_pred_train)
test_f1 = f1_score(y_test, y_pred_test)
test_roc_auc = roc_auc_score(y_test, y_pred_test)

print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")
print(f"Train Precision: {train_precision}")
print(f"Test Precision: {test_precision}")
print(f"Train Recall: {train_recall}")
print(f"Test Recall: {test_recall}")
print(f"Train F1 Score: {train_f1}")
print(f"Test F1 Score: {test_f1}")
print(f"Test ROC AUC: {test_roc_auc}")

# Classification report
print("Classification Report:")
print(classification_report(y_test, y_pred_test))

# Plot confusion matrix
ConfusionMatrixDisplay.from_estimator(dtree, X_test, y_test)
plt.title("Confusion Matrix")
plt.show()

# Visualize the Decision Tree
plt.figure(figsize=(20, 10))
tree.plot_tree(dtree, feature_names=X.columns, class_names=['Incorrect', 'Correct'], filled=True)
plt.show()

"""From the confusion matrix we can see that the logistic model is better than the decision tree in identifying false values.

The model lack precision in predicting false values. We could try to use random forest or gradient boosting to improve results.
"""

freq_correct = pd.crosstab(index=final_merged_df['is_correct'], columns=[final_merged_df['difficulty'],final_merged_df['learning_stage']],margins=True)
freq_correct

freq_correct/freq_correct.loc['All']

# percentage of correct answers
total_answers = final_merged_df.shape[0]  # Total number of answers
correct_answers = final_merged_df['is_correct'].sum()  # Number of correct answers

percentage_correct = (correct_answers / total_answers) * 100

print("Percentage of correct answers:", percentage_correct)

#Classification
#Goal: Predict a categorical outcome.
#
#Example: Predict whether a student will solve an exercise correctly based on various features (e.g., time taken, number of exercises solved previously, etc.).
#Possible Features:
#
#Time taken to solve the exercise (numeric)
#Number of exercises solved by the student (numeric)
#Time of day or day of the week (derived from the timestamp)
#Target Variable:
#
#is_correct (True/False)
#Approach:
#
#Use classification algorithms like Logistic Regression, Decision Trees, Random Forest

#Regression
#Goal: Predict a numeric outcome.
#
#Example: Predict the time taken to solve an exercise based on whether it was solved correctly and other features.
#Possible Features:
#
#Whether the exercise was solved correctly (categorical, need to convert to numeric)
#Number of exercises solved by the student (numeric)
#Time of day or day of the week (derived from the timestamp)
#Target Variable:
#
#time_taken (numeric)
#Approach:
#
#Use regression algorithms like Linear Regression, Decision Trees, Random Forest, etc.

#Exploratory Data Analysis (EDA):

#

#Understand the structure and distribution of the data.

#Identify relationships, patterns, and anomalies.

#Visualize data distributions, correlations, and trends.

#Handle missing values and outliers if necessary.

#Feature Engineering:

#

#Create new features from existing data.

#Encode categorical variables if needed.

#Select or extract features relevant to the model.

#Splitting the Data:

#

#Split the data into training and testing sets (and sometimes a validation set).

#Normalization/Scaling:

#

#Apply normalization or scaling techniques to standardize the feature values.

#This step is done after splitting the data to prevent data leakage.

#Model Training:

#

#Train the machine learning models using the preprocessed data.

#Model Evaluation:

#

#Evaluate model performance using appropriate metrics.

#Hyperparameter Tuning:

#

#Optimize the model by tuning its hyperparameters.

#Model Deployment:

#

#Deploy the final model for production use.